{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import mmh3\n",
    "import numpy as np\n",
    "import itertools\n",
    "import collections\n",
    "import pandas as pd\n",
    "import more_itertools\n",
    "from itertools import tee\n",
    "import re\n",
    "#################### Utilities ######################\n",
    "#hashes a list of strings\n",
    "def listhash(l,seed):\n",
    "\tval = 0\n",
    "\tfor e in l:\n",
    "\t\tval = val ^ mmh3.hash(e, seed)\n",
    "\treturn val \n",
    "\n",
    "################### Similarity ######################\n",
    "\n",
    "docs = {} #dictionary mapping document id to document contents\n",
    "\n",
    "data = pd.read_csv(r\"C:\\Users\\hasee\\Documents\\comptools\\DTU_CTDS\\data\\articles1.csv\")\n",
    "\n",
    "data1 = data[:50000]\n",
    "key = data1[\"Unnamed: 0\"]\n",
    "data2 = data1[\"content\"]\n",
    "docs = data2.to_dict()\n",
    "# docs = data2.tolist()\n",
    "# g_docs = data2.tolist()\n",
    "docs = list(zip(data2.tolist())) ## convert to tuple for starmap to work basically getting ['dfdf', 'dddd', 'sss'] => [('dfdf'), ('dddd'), ('sss')]\n",
    "# print(docs[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Extension of standard pairwise function to 3-pairwise from the py standard lib'''\n",
    "def three_pairwise(iterable):\n",
    "    # pairwise('ABCDEFG') --> AB BC CD DE EF FG\n",
    "    # three_pairwise('ABCDEFG') --> ABC BCD CDE DEF EFG FG\n",
    "    a, b, c = tee(iterable, 3) ## Create three iterators\n",
    "    next(b, None) ## Advance the second\n",
    "    next(c, None) ## Advance the third\n",
    "    next(c, None) ## Advance the third once again. This ensures the third iterator starts at the third element and that we can create the 3-tuple\n",
    "    return zip(a, b, c) ### Zip everything (1 elem, 2 elem, and 3 elem) concurrently.\n",
    "\n",
    "def hash_docs(doc, k=50):\n",
    "    # three_tup=zip\n",
    "    lst_shingles_h=[]\n",
    "    # for doc in docs:\n",
    "    seed=1\n",
    "    three_tup=three_pairwise(doc.split(\" \"))\n",
    "\n",
    "    three_tup=list(three_tup)\n",
    "\n",
    "    three_tup=[elem for elem in three_tup if '' not in elem]\n",
    "    \n",
    "    # for t in three_tup:\n",
    "    #     if '' or ' ' in t:\n",
    "    #         print(\"single q\", t)\n",
    "    #     if \" \" or \"\" in t:\n",
    "    #         print(\"double \", t)\n",
    "\n",
    "    # print(set(three_tup))\n",
    "\n",
    "    # seeds = [seed+i for i in range(0,k)]\n",
    "    ret_lst=[]\n",
    "    # for s in seeds:  \n",
    "    #     ret_lst.append(np.array(min([listhash(shingle, s) for shingle in three_tup], default=0)))  \n",
    "    # lens=[]\n",
    "    # my_lst=[]\n",
    "    # for shingle in three_tup:\n",
    "    #     my_lst.append(listhash(shingle, seed))\n",
    "    #     lens.append(len(my_lst))\n",
    "    # ret_lst.append(np.array(sorted(my_lst)[:k]))\n",
    "    # print(\"min len\", min(lens))\n",
    "    # ret_lst.append(np.array(sorted([listhash(shingle, seed) for shingle in three_tup])[:k]))\n",
    "    # print(\"min leng\", min(lens))\n",
    "    \n",
    "    # s_lst = sorted([listhash(shingle, seed) for shingle in three_tup])\n",
    "    # print(len(s_lst))\n",
    "    return np.array(sorted([listhash(shingle, seed) for shingle in three_tup]))##[:k])#, len(s_lst)\n",
    "\n",
    "''' Returns document as a list of hashes'''\n",
    "'''\n",
    "Creates shingles of size q, removing shingles of size < q. Removes duplicates and hashes the result.\n",
    "'''\n",
    "def hashed_lst_shingles(my_docs):\n",
    "\n",
    "\n",
    "    hashed_docs = itertools.starmap(hash_docs, my_docs)\n",
    "    return hashed_docs\n",
    "\n",
    "# doc = \"You and me, we made a vow. For better or for worse. I can't believe you let me down\"\n",
    "# lst_shnigles = hashed_lst_shingles(3, doc)\n",
    "# print(lst_shnigles)\n",
    "# [['You', 'and', 'me,'], ['and', 'me,', 'we'], ['me,', 'we', 'made'], ['we', 'made', 'a'], ['made', 'a', 'vow.'], ['a', 'vow.', 'For'], ['vow.', 'For', 'better'], ['For', 'better', 'or'], ['better', 'or', 'for'], ['or', 'for', 'worse.'], ['for', 'worse.', 'I'], ['worse.', 'I', \"can't\"], ['I', \"can't\", 'believe'], [\"can't\", 'believe', 'you'], ['believe', 'you', 'let'], ['you', 'let', 'me'], ['let', 'me', 'down']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MinHashing implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49920,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hasee\\AppData\\Local\\Temp\\ipykernel_12596\\1858395464.py:9: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  sig_m=np.array(hashed_shingles).T\n"
     ]
    }
   ],
   "source": [
    "# test_docs=list(zip([\"You and me, we made a vow. For better or for worse. I can't believe you let me down\",\n",
    "# \"Time, space and state. Equal everything explanable.\",\n",
    "# \"You and me, we made a vow. For better or for worse. I can't believe you let me down\"]))\n",
    "hashed_shingles=hashed_lst_shingles(docs)\n",
    "# print(hash)\n",
    "hashed_shingles = list(hashed_shingles)\n",
    "hashed_shingles=[lst for lst in hashed_shingles if len(lst)>0]\n",
    "# hashed_shingles\n",
    "sig_m=np.array(hashed_shingles).T\n",
    "print(sig_m.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, 49920)"
      ]
     },
     "execution_count": 459,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lens=min([len(x) for x in sig_m])\n",
    "print(lens)\n",
    "num_docs=sig_m.shape[0]\n",
    "for i in range(num_docs):\n",
    "    sig_m[i]=sig_m[i][:lens]\n",
    "sig_m=np.stack(sig_m).T\n",
    "sig_m.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_s=[]\n",
    "# for i in range(len(hashed_shingles)):\n",
    "#     my_s.append(hashed_shingles[i][1])\n",
    "# min(my_s)\n",
    "# # min(my_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSH implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Jaccard similarity'''\n",
    "def jaccard(s1, s2):\n",
    "    return len(s1 & s2) / len(s1 | s2)\n",
    "\n",
    "''' Implementation of LSH, dividing signature matrix into b band with r rows each'''\n",
    "def LSH(sig_m, b, r):\n",
    "    b=20\n",
    "    r=5\n",
    "    #b*r=num_hash_funcs\n",
    "\n",
    "    sim_hashes=[]\n",
    "    start=0\n",
    "    for i in range(b):\n",
    "        sim_hashes.append([listhash(col, seed=i) for col in sig_m[start:start+r,:].T])\n",
    "        start=i+r\n",
    "\n",
    "    return sim_hashes\n",
    "\n",
    "''' Find candidate pairs by checking to see if the hashes match.\n",
    "Then we check to see that the Jaccard similarity b/w each pair of docs is atleast t. If so, we consider it a candidate pair otherwise not '''\n",
    "def get_cand_pairs(sim_hashes, t):\n",
    "    cand_pairs=set()\n",
    "    for L in sim_hashes:\n",
    "        dups = collections.defaultdict(list)\n",
    "        for i, e in enumerate(L):\n",
    "            dups[e].append(i)\n",
    "        for _, v in sorted(dups.items()):\n",
    "            if len(v) >= 2:\n",
    "                cand_pairs.add(tuple(v))\n",
    "    cand_pairs=list(cand_pairs)\n",
    "    filtered_cand_pairs = [pair for pair in cand_pairs if (jaccard(set(sig_m[:, pair[0]]), set(sig_m[:, pair[1]])) > t)]\n",
    "    return filtered_cand_pairs\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4094"
      ]
     },
     "execution_count": 462,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b=20\n",
    "r=5\n",
    "sim_hashes = LSH(sig_m, b=20, r=5)\n",
    "pairs=get_cand_pairs(sim_hashes, t=(1/b)**(1/r))\n",
    "len(pairs)\n",
    "# pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen using LSH, we find that documents 5 and 6 are similar."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 ('comptools')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e991eec70dcdfedc5f1bcac3fae689529e9bd44f9e597bb7444204679111271f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
