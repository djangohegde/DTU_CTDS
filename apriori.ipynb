{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hasee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hasee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\hasee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read document calltounconv00baxt.txt\n",
      "read document gospeltruth00whit.txt\n",
      "read document lifeofrevrichard00baxt.txt\n",
      "read document memoirjamesbrai00ricegoog.txt\n",
      "read document practicalthought00nev.txt\n",
      "read document remember00palm.txt\n",
      "read document remembermeorholy00palm.txt\n",
      "read document thoughtsonpopery00nevi.txt\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import mmh3\n",
    "import numpy as np\n",
    "import itertools\n",
    "import collections\n",
    "from efficient_apriori import apriori\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"words\")\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import words\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "#################### Utilities ######################\n",
    "#hashes a list of strings\n",
    "def listhash(l,seed):\n",
    "\tval = 0\n",
    "\tfor e in l:\n",
    "\t\tval = val ^ mmh3.hash(e, seed)\n",
    "\treturn val \n",
    "\n",
    "################### Similarity ######################\n",
    "\n",
    "df1 = pd.read_csv(r\"C:\\Users\\hasee\\Documents\\comptools\\project\\data\\articles1.csv\") #TODO replace with sys path\n",
    "df2 = pd.read_csv(r\"C:\\Users\\hasee\\Documents\\comptools\\project\\data\\articles2.csv\")\n",
    "df3 = pd.read_csv(r\"C:\\Users\\hasee\\Documents\\comptools\\project\\data\\articles3.csv\")\n",
    "\n",
    "df1=df1[\"content\"]\n",
    "df2=df2[\"content\"]\n",
    "df3=df3[\"content\"]\n",
    "\n",
    "###############################\n",
    "# pd.read_csv(r\"C:\\Users\\hasee\\Documents\\comptools\\week5\\DataWeek5\\ats_corpus_small.csv\")\n",
    "docs = {} #dictionary mapping document id to document contents\n",
    "\n",
    "datafolder = os.path.join(\"C:\\\\Users\\\\hasee\\\\Documents\\\\comptools\\\\week5\\\\DataWeek5\", \"ats_corpus_small\")   # change to ats_corpus for large data set\n",
    "lst=[]\n",
    "for file in os.listdir(datafolder):\n",
    "    filepath = os.path.join(datafolder, file)\n",
    "    f = open(filepath, 'r')\n",
    "    docs[file] = f.read()\n",
    "    # tmp = f.read()\n",
    "    # lst.append(pd.DataFrame(tmp))\n",
    "    print(\"read document \" + file)\n",
    "    f.close()\n",
    "\n",
    "# df_lst=df1.values.tolist()\n",
    "\n",
    "# transactions = [tuple(row.split()) for row in df1.values.tolist()]\n",
    "# transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    \\nGlass. \\n\\n\\n\\nBook,____._._ \\n\\n\\n\\n(ttmmti...\n",
       "1    \\n\\n\\nCvCC \\n\\n\\n\\na c: CIS: \\n\\nâ–  <Â£ <tC. ...\n",
       "2    T H E L I F E \\n\\n\\n\\nor \\n\\n\\n\\n\\nREV. RICHAR...\n",
       "3    Google \\n\\n\\n\\nThis is a digital copy of a boo...\n",
       "4    LIBEAEY \\n\\n^biological Â£eminarg, \\n\\n\\n\\nPRI...\n",
       "5    Remember \\nBy \\nRat Palmer. \\nBoston: \\n\\nTHE ...\n",
       "6    //Wf \\n\\n\\n\\n'/^y /L.-*^ \\n\\n\\n\\nJ?i.{^, \\n\\n\\...\n",
       "7    L I B H -A. n \"sr \\n\\nPBINCETOK. y. J. \\nThe S...\n",
       "dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_small = pd.DataFrame(docs, index=[0,1,2,3,4,5,6,7])\n",
    "df_small = pd.Series(data=docs)\n",
    "df_small = df_small.set_axis([0,1,2,3,4,5,6,7])\n",
    "# df_small_series = pd.Series(df_small)\n",
    "df_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"also one\" in df1[1]\n",
    "# df1[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean the content, remove stop words,punctuatio from the content\n",
    "# and normalize each word\n",
    "def clean(text):\n",
    "    stop = set(stopwords.words('english'))\n",
    "    # words_set = set(words.words())\n",
    "    \"\"\"\n",
    "    This function takes as input a text on which several \n",
    "    NLTK algorithms will be applied in order to preprocess it\n",
    "    \"\"\"\n",
    "    # Removing numbers\n",
    "    # print(\"1 start\")\n",
    "    document = re.sub(r'\\d+', ' ', text)\n",
    "    # print(\"2 start\")\n",
    "    # Removing special characters\n",
    "    document = re.sub(r\"[^a-zA-Z0-9]+\", ' ', document)\n",
    "    # print(\"3 start\")\n",
    "    tokens = word_tokenize(document)\n",
    "    # Remove the punctuations\n",
    "    # print(\"4 start\")\n",
    "    tokens = [word for word in tokens if word not in stop and len(word) > 2]\n",
    "    # Lower the tokens\n",
    "    # print(\"5 start\")\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    # Remove stopword\n",
    "    # print(\"6 start\")\n",
    "    tokens = [word for word in tokens if not word in stop]\n",
    "    # remove common  words\n",
    "    # tokens = [word for word in tokens if not word in words_set]\n",
    "    # Lemmatize\n",
    "    # lemma = WordNetLemmatizer()\n",
    "    # tokens = [lemma.lemmatize(word, pos = \"v\") for word in tokens]\n",
    "    # tokens = [lemma.lemmatize(word, pos = \"n\") for word in tokens]\n",
    "    return tuple(tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import multiprocessing\n",
    "# try:\n",
    "#     cpus = multiprocessing.cpu_count()\n",
    "#     print(cpus)\n",
    "# except NotImplementedError:\n",
    "#     cpus = 2   # arbitrary default\n",
    "\n",
    "# def square(n):\n",
    "#     return n * n\n",
    "\n",
    "# pool = multiprocessing.Pool(processes=cpus)\n",
    "# ret = pool.map(square, range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49999\n",
      "42571\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "print(len(df2))\n",
    "print(len(df3))\n",
    "print(len(df_small))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transactions = [clean(article) for article in df2.values.tolist()]\n",
    "transactions = [tuple(row.split()) for row in df1.values.tolist()]\n",
    "# transactions = [clean(article) for article in df_small.values.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transactions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [32], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m itemsets, rules \u001b[39m=\u001b[39m apriori(transactions[:\u001b[39m10\u001b[39m], min_support\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m, min_confidence\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m, output_transaction_ids\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\hasee\\pyenvs\\comptools\\lib\\site-packages\\efficient_apriori\\apriori.py:58\u001b[0m, in \u001b[0;36mapriori\u001b[1;34m(transactions, min_support, min_confidence, max_length, verbosity, output_transaction_ids)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapriori\u001b[39m(\n\u001b[0;32m     13\u001b[0m     transactions: typing\u001b[39m.\u001b[39mIterable[typing\u001b[39m.\u001b[39mUnion[\u001b[39mset\u001b[39m, \u001b[39mtuple\u001b[39m, \u001b[39mlist\u001b[39m]],\n\u001b[0;32m     14\u001b[0m     min_support: \u001b[39mfloat\u001b[39m \u001b[39m=\u001b[39m \u001b[39m0.5\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     18\u001b[0m     output_transaction_ids: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m     19\u001b[0m ):\n\u001b[0;32m     20\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[39m    The classic apriori algorithm as described in 1994 by Agrawal et al.\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[39m    [{a} -> {b}]\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 58\u001b[0m     itemsets, num_trans \u001b[39m=\u001b[39m itemsets_from_transactions(\n\u001b[0;32m     59\u001b[0m         transactions,\n\u001b[0;32m     60\u001b[0m         min_support,\n\u001b[0;32m     61\u001b[0m         max_length,\n\u001b[0;32m     62\u001b[0m         verbosity,\n\u001b[0;32m     63\u001b[0m         output_transaction_ids\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m     64\u001b[0m     )\n\u001b[0;32m     66\u001b[0m     itemsets_raw \u001b[39m=\u001b[39m {\n\u001b[0;32m     67\u001b[0m         length: {item: counter\u001b[39m.\u001b[39mitemset_count \u001b[39mfor\u001b[39;00m (item, counter) \u001b[39min\u001b[39;00m itemsets\u001b[39m.\u001b[39mitems()}\n\u001b[0;32m     68\u001b[0m         \u001b[39mfor\u001b[39;00m (length, itemsets) \u001b[39min\u001b[39;00m itemsets\u001b[39m.\u001b[39mitems()\n\u001b[0;32m     69\u001b[0m     }\n\u001b[0;32m     70\u001b[0m     rules \u001b[39m=\u001b[39m generate_rules_apriori(itemsets_raw, min_confidence, num_trans, verbosity)\n",
      "File \u001b[1;32mc:\\Users\\hasee\\pyenvs\\comptools\\lib\\site-packages\\efficient_apriori\\itemsets.py:319\u001b[0m, in \u001b[0;36mitemsets_from_transactions\u001b[1;34m(transactions, min_support, max_length, verbosity, output_transaction_ids)\u001b[0m\n\u001b[0;32m    314\u001b[0m itemsets_list \u001b[39m=\u001b[39m \u001b[39msorted\u001b[39m(item \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m large_itemsets[k \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mkeys())\n\u001b[0;32m    316\u001b[0m \u001b[39m# Gen candidates of length k + 1 by joining, prune, and copy as set\u001b[39;00m\n\u001b[0;32m    317\u001b[0m \u001b[39m# This algorithm assumes that the list of itemsets are sorted,\u001b[39;00m\n\u001b[0;32m    318\u001b[0m \u001b[39m# and that the itemsets themselves are sorted tuples\u001b[39;00m\n\u001b[1;32m--> 319\u001b[0m C_k: typing\u001b[39m.\u001b[39mList[\u001b[39mtuple\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39;49m(apriori_gen(itemsets_list))\n\u001b[0;32m    321\u001b[0m \u001b[39mif\u001b[39;00m verbosity \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    322\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m  Found \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m candidate itemsets of length \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mlen\u001b[39m(C_k), k))\n",
      "File \u001b[1;32mc:\\Users\\hasee\\pyenvs\\comptools\\lib\\site-packages\\efficient_apriori\\itemsets.py:225\u001b[0m, in \u001b[0;36mapriori_gen\u001b[1;34m(itemsets)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \u001b[39mCompute all possible k + 1 length supersets from k length itemsets.\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[39m[(1, 2, 3, 4)]\u001b[39;00m\n\u001b[0;32m    223\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    224\u001b[0m possible_extensions \u001b[39m=\u001b[39m join_step(itemsets)\n\u001b[1;32m--> 225\u001b[0m \u001b[39myield from\u001b[39;00m prune_step(itemsets, possible_extensions)\n",
      "File \u001b[1;32mc:\\Users\\hasee\\pyenvs\\comptools\\lib\\site-packages\\efficient_apriori\\itemsets.py:187\u001b[0m, in \u001b[0;36mprune_step\u001b[1;34m(itemsets, possible_itemsets)\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[39mfor\u001b[39;00m possible_itemset \u001b[39min\u001b[39;00m possible_itemsets:\n\u001b[0;32m    180\u001b[0m \n\u001b[0;32m    181\u001b[0m     \u001b[39m# Remove 1 from the combination, same as k-1 combinations\u001b[39;00m\n\u001b[0;32m    182\u001b[0m     \u001b[39m# The itemsets created by removing the last two items in the possible\u001b[39;00m\n\u001b[0;32m    183\u001b[0m     \u001b[39m# itemsets must be part of the itemsets by definition,\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     \u001b[39m# due to the way the `join_step` function merges the sorted itemsets\u001b[39;00m\n\u001b[0;32m    186\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(possible_itemset) \u001b[39m-\u001b[39m \u001b[39m2\u001b[39m):\n\u001b[1;32m--> 187\u001b[0m         removed \u001b[39m=\u001b[39m possible_itemset[:i] \u001b[39m+\u001b[39m possible_itemset[i \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m :]\n\u001b[0;32m    189\u001b[0m         \u001b[39m# If every k combination exists in the set of itemsets,\u001b[39;00m\n\u001b[0;32m    190\u001b[0m         \u001b[39m# yield the possible itemset. If it does not exist, then it's\u001b[39;00m\n\u001b[0;32m    191\u001b[0m         \u001b[39m# support cannot be large enough, since supp(A) >= supp(AB) for\u001b[39;00m\n\u001b[0;32m    192\u001b[0m         \u001b[39m# all B, and if supp(S) is large enough, then supp(s) must be large\u001b[39;00m\n\u001b[0;32m    193\u001b[0m         \u001b[39m# enough for every s which is a subset of S.\u001b[39;00m\n\u001b[0;32m    194\u001b[0m         \u001b[39m# This is the downward-closure property of the support function.\u001b[39;00m\n\u001b[0;32m    195\u001b[0m         \u001b[39mif\u001b[39;00m removed \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m itemsets:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "itemsets, rules = apriori(transactions[:10], min_support=0.1, min_confidence=0.1, output_transaction_ids=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The', 'of', 'the')\n",
      "('The', 'the', 'to')\n",
      "('a', 'and', 'for')\n",
      "('a', 'and', 'in')\n",
      "('a', 'and', 'is')\n",
      "('a', 'and', 'of')\n",
      "('a', 'and', 'on')\n",
      "('a', 'and', 'that')\n",
      "('a', 'and', 'the')\n",
      "('a', 'and', 'to')\n",
      "('a', 'and', 'with')\n",
      "('a', 'for', 'in')\n",
      "('a', 'for', 'is')\n",
      "('a', 'for', 'of')\n",
      "('a', 'for', 'on')\n",
      "('a', 'for', 'that')\n",
      "('a', 'for', 'the')\n",
      "('a', 'for', 'to')\n",
      "('a', 'in', 'is')\n",
      "('a', 'in', 'of')\n",
      "('a', 'in', 'on')\n",
      "('a', 'in', 'that')\n",
      "('a', 'in', 'the')\n",
      "('a', 'in', 'to')\n",
      "('a', 'in', 'with')\n",
      "('a', 'is', 'of')\n",
      "('a', 'is', 'on')\n",
      "('a', 'is', 'that')\n",
      "('a', 'is', 'the')\n",
      "('a', 'is', 'to')\n",
      "('a', 'of', 'on')\n",
      "('a', 'of', 'that')\n",
      "('a', 'of', 'the')\n",
      "('a', 'of', 'to')\n",
      "('a', 'of', 'with')\n",
      "('a', 'on', 'that')\n",
      "('a', 'on', 'the')\n",
      "('a', 'on', 'to')\n",
      "('a', 'on', 'with')\n",
      "('a', 'that', 'the')\n",
      "('a', 'that', 'to')\n",
      "('a', 'that', 'with')\n",
      "('a', 'the', 'to')\n",
      "('a', 'the', 'with')\n",
      "('a', 'to', 'with')\n",
      "('and', 'for', 'in')\n",
      "('and', 'for', 'is')\n",
      "('and', 'for', 'of')\n",
      "('and', 'for', 'on')\n",
      "('and', 'for', 'that')\n",
      "('and', 'for', 'the')\n",
      "('and', 'for', 'to')\n",
      "('and', 'in', 'is')\n",
      "('and', 'in', 'of')\n",
      "('and', 'in', 'on')\n",
      "('and', 'in', 'that')\n",
      "('and', 'in', 'the')\n",
      "('and', 'in', 'to')\n",
      "('and', 'in', 'with')\n",
      "('and', 'is', 'of')\n",
      "('and', 'is', 'on')\n",
      "('and', 'is', 'that')\n",
      "('and', 'is', 'the')\n",
      "('and', 'is', 'to')\n",
      "('and', 'of', 'on')\n",
      "('and', 'of', 'that')\n",
      "('and', 'of', 'the')\n",
      "('and', 'of', 'to')\n",
      "('and', 'of', 'with')\n",
      "('and', 'on', 'that')\n",
      "('and', 'on', 'the')\n",
      "('and', 'on', 'to')\n",
      "('and', 'on', 'with')\n",
      "('and', 'that', 'the')\n",
      "('and', 'that', 'to')\n",
      "('and', 'that', 'with')\n",
      "('and', 'the', 'to')\n",
      "('and', 'the', 'with')\n",
      "('and', 'to', 'with')\n",
      "('for', 'in', 'is')\n",
      "('for', 'in', 'of')\n",
      "('for', 'in', 'on')\n",
      "('for', 'in', 'that')\n",
      "('for', 'in', 'the')\n",
      "('for', 'in', 'to')\n",
      "('for', 'is', 'of')\n",
      "('for', 'is', 'the')\n",
      "('for', 'is', 'to')\n",
      "('for', 'of', 'on')\n",
      "('for', 'of', 'that')\n",
      "('for', 'of', 'the')\n",
      "('for', 'of', 'to')\n",
      "('for', 'on', 'that')\n",
      "('for', 'on', 'the')\n",
      "('for', 'on', 'to')\n",
      "('for', 'that', 'the')\n",
      "('for', 'that', 'to')\n",
      "('for', 'the', 'to')\n",
      "('in', 'is', 'of')\n",
      "('in', 'is', 'on')\n",
      "('in', 'is', 'that')\n",
      "('in', 'is', 'the')\n",
      "('in', 'is', 'to')\n",
      "('in', 'of', 'on')\n",
      "('in', 'of', 'that')\n",
      "('in', 'of', 'the')\n",
      "('in', 'of', 'to')\n",
      "('in', 'of', 'with')\n",
      "('in', 'on', 'that')\n",
      "('in', 'on', 'the')\n",
      "('in', 'on', 'to')\n",
      "('in', 'on', 'with')\n",
      "('in', 'that', 'the')\n",
      "('in', 'that', 'to')\n",
      "('in', 'the', 'to')\n",
      "('in', 'the', 'with')\n",
      "('in', 'to', 'with')\n",
      "('is', 'of', 'on')\n",
      "('is', 'of', 'that')\n",
      "('is', 'of', 'the')\n",
      "('is', 'of', 'to')\n",
      "('is', 'on', 'that')\n",
      "('is', 'on', 'the')\n",
      "('is', 'on', 'to')\n",
      "('is', 'that', 'the')\n",
      "('is', 'that', 'to')\n",
      "('is', 'the', 'to')\n",
      "('of', 'on', 'that')\n",
      "('of', 'on', 'the')\n",
      "('of', 'on', 'to')\n",
      "('of', 'on', 'with')\n",
      "('of', 'that', 'the')\n",
      "('of', 'that', 'to')\n",
      "('of', 'that', 'with')\n",
      "('of', 'the', 'to')\n",
      "('of', 'the', 'with')\n",
      "('of', 'to', 'with')\n",
      "('on', 'that', 'the')\n",
      "('on', 'that', 'to')\n",
      "('on', 'the', 'to')\n",
      "('on', 'the', 'with')\n",
      "('on', 'to', 'with')\n",
      "('that', 'the', 'to')\n",
      "('that', 'the', 'with')\n",
      "('that', 'to', 'with')\n",
      "('the', 'to', 'with')\n"
     ]
    }
   ],
   "source": [
    "len(itemsets)\n",
    "# for key in itemsets[3].keys():\n",
    "#     print(key)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 ('comptools')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e991eec70dcdfedc5f1bcac3fae689529e9bd44f9e597bb7444204679111271f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
