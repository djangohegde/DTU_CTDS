{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/kovalsky/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/kovalsky/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Wed Nov 23 20:27:23 2022\n",
    "\n",
    "@author: geng8\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk as nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "import string\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from wordcloud import WordCloud\n",
    "from wordcloud import STOPWORDS\n",
    "import matplotlib.pyplot as plt \n",
    "from collections import defaultdict\n",
    "from efficient_apriori import apriori\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess our data\n",
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation)\n",
    "#our data have a special punctuation '—' , need to be removed.\n",
    "exclude.add('—')\n",
    "exclude.add('“')\n",
    "exclude.add('”')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize word normalizer and data loader\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "data = pd.read_csv(\"/home/kovalsky/DTU_CTDS/data/articles1.csv\", sep=',')\n",
    "\n",
    "data_content = data[\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean the content, remove stop words,punctuatio from the content\n",
    "# and normalize each word\n",
    "def clean(text):\n",
    "    \"\"\"\n",
    "    This function takes as input a text on which several \n",
    "    NLTK algorithms will be applied in order to preprocess it\n",
    "    \"\"\"\n",
    "  \n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove the punctuations\n",
    "    tokens = [word for word in tokens if word not in stop and len(word) > 2]\n",
    "    # Lower the tokens\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    # Remove stopword\n",
    "    tokens = [word for word in tokens if not word in stop]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6264/3726915952.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_content.loc[i] = str([' '.join(cleaned)])\n"
     ]
    }
   ],
   "source": [
    "#replace the original content with cleaned content \n",
    "for i in data_content.index:  \n",
    "    cleaned = clean(data_content[i])   \n",
    "    data_content.loc[i] = str([' '.join(cleaned)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count Vectoriser then tidf transformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(data_content)\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "transformer = TfidfTransformer(smooth_idf=False)\n",
    "tfidf = transformer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 2 ... 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "num_clusters = 3\n",
    "km = DBSCAN(eps=num_clusters, min_samples=2)\n",
    "km.fit(tfidf)\n",
    "print(km.labels_)\n",
    "clusters = km.labels_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster0 = []\n",
    "cluster1 = []\n",
    "cluster2 = []\n",
    "\n",
    "for i in clusters:\n",
    "    if clusters[i] == 0:\n",
    "        cluster0.append(data_content[i])\n",
    "\n",
    "    if clusters[i] == 1:\n",
    "        cluster1.append(data_content[i])\n",
    "\n",
    "    if clusters[i] == 2:\n",
    "        cluster2.append(data_content[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/kovalsky/DTU_CTDS/cluster.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/kovalsky/DTU_CTDS/cluster.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m cluster0_plag \u001b[39m=\u001b[39m []\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/kovalsky/DTU_CTDS/cluster.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m cluster0:\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/kovalsky/DTU_CTDS/cluster.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39m# cluster0_plag = metrics.pairwise.cosine_similarity(cluster0[i], cluster0[i+1]) \u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/kovalsky/DTU_CTDS/cluster.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39m# print(i)\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/kovalsky/DTU_CTDS/cluster.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     metrics\u001b[39m.\u001b[39mpairwise\u001b[39m.\u001b[39mcosine_similarity(cluster0[i], cluster0[i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m])\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "cluster0_plag = []\n",
    "\n",
    "for i in cluster0:\n",
    "    # cluster0_plag = metrics.pairwise.cosine_similarity(cluster0[i], cluster0[i+1]) \n",
    "    # print(i)\n",
    "    metrics.pairwise.cosine_similarity(cluster0[i], cluster0[i+1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
